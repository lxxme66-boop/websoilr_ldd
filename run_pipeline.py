#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬QAç”Ÿæˆç³»ç»Ÿ - ç»Ÿä¸€æµæ°´çº¿è„šæœ¬
æ•´åˆäº†æ•°æ®å¬å›ã€æ¸…ç†ã€QAç”Ÿæˆå’Œè´¨é‡æ§åˆ¶çš„å®Œæ•´æµç¨‹
"""

import os
import sys
import json
import argparse
import asyncio
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Union

# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°ç³»ç»Ÿè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å„æ¨¡å—
from doubao_main_batch_inference import proccess_folders
from clean_data import clean_process
from text_qa_generation.text_qa_generation import main as qa_generation_main
from qwen_argument import main as qwen_argument_main

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class IntelligentQAPipeline:
    """æ™ºèƒ½QAç”Ÿæˆæµæ°´çº¿ä¸»ç±»"""
    
    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.config = self.load_config(config_path)
        self.setup_directories()
        
    def load_config(self, config_path: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.warning(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return self.get_default_config()
    
    def get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "api": {
                "ark_url": "http://0.0.0.0:8080/v1",
                "api_key": "ae37bba4-73be-4c22-b1a7-c6b1f5ec3a4b"
            },
            "models": {
                "default_model": "/mnt/storage/models/Skywork/Skywork-R1V3-38B",
                "qa_generator_model": {
                    "path": "/mnt/storage/models/Skywork/Skywork-R1V3-38B",
                    "type": "api"
                }
            },
            "processing": {
                "batch_size": 100,
                "max_concurrent": 20,
                "quality_threshold": 0.7,
                "selected_task_number": 1000
            },
            "domains": {
                "semiconductor": {
                    "prompts": [343, 3431, 3432],
                    "keywords": ["IGZO", "TFT", "OLED", "åŠå¯¼ä½“"],
                    "quality_criteria": "high"
                },
                "optics": {
                    "prompts": [343, 3431, 3432],
                    "keywords": ["å…‰è°±", "å…‰å­¦", "æ¿€å…‰"],
                    "quality_criteria": "high"
                }
            }
        }
    
    def setup_directories(self):
        """è®¾ç½®å¿…è¦çš„ç›®å½•"""
        directories = [
            "data/input",
            "data/retrieved", 
            "data/cleaned",
            "data/qa_results",
            "data/final_output",
            "logs",
            "temp"
        ]
        
        for directory in directories:
            Path(directory).mkdir(parents=True, exist_ok=True)
            
    async def run_data_retrieval(self, input_path: str, output_path: str, 
                                index: int = 43, **kwargs) -> str:
        """è¿è¡Œæ•°æ®å¬å›é˜¶æ®µ"""
        logger.info("=== å¼€å§‹æ•°æ®å¬å›é˜¶æ®µ ===")
        
        try:
            # è®¾ç½®å‚æ•°
            folders = os.listdir(input_path) if os.path.isdir(input_path) else [input_path]
            storage_folder = output_path
            temporary_folder = kwargs.get('temporary_folder', 'TEMP')
            maximum_tasks = kwargs.get('batch_size', self.config['processing']['batch_size'])
            selected_task_number = kwargs.get('selected_task_number', 
                                            self.config['processing']['selected_task_number'])
            
            # æ‰§è¡Œå¬å›
            results = await proccess_folders(
                folders=folders,
                pdf_path=input_path,
                temporary_folder=temporary_folder,
                index=index,
                maximum_tasks=maximum_tasks,
                selected_task_number=selected_task_number
            )
            
            # ä¿å­˜ç»“æœ
            output_file = os.path.join(storage_folder, "total_response.pkl")
            with open(output_file, "wb") as f:
                import pickle
                pickle.dump(results, f)
                
            logger.info(f"æ•°æ®å¬å›å®Œæˆï¼Œå…±å¤„ç† {len(results)} æ¡æ•°æ®")
            logger.info(f"ç»“æœä¿å­˜è‡³: {output_file}")
            
            return output_file
            
        except Exception as e:
            logger.error(f"æ•°æ®å¬å›é˜¶æ®µå‡ºé”™: {str(e)}")
            raise
    
    def run_data_cleaning(self, input_file: str, output_path: str, **kwargs) -> str:
        """è¿è¡Œæ•°æ®æ¸…ç†é˜¶æ®µ"""
        logger.info("=== å¼€å§‹æ•°æ®æ¸…ç†é˜¶æ®µ ===")
        
        try:
            copy_parsed_pdf = kwargs.get('copy_parsed_pdf', False)
            
            # æ‰§è¡Œæ¸…ç†
            clean_process(
                input_file=input_file,
                output_file=output_path,
                copy_parsed_pdf=copy_parsed_pdf
            )
            
            cleaned_file = os.path.join(output_path, "total_response.json")
            logger.info(f"æ•°æ®æ¸…ç†å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {cleaned_file}")
            
            return cleaned_file
            
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†é˜¶æ®µå‡ºé”™: {str(e)}")
            raise
    
    async def run_qa_generation(self, input_file: str, output_path: str, 
                               index: int = 343, **kwargs) -> str:
        """è¿è¡ŒQAç”Ÿæˆé˜¶æ®µ"""
        logger.info("=== å¼€å§‹QAç”Ÿæˆé˜¶æ®µ ===")
        
        try:
            # æ„é€ å‚æ•°
            args_dict = {
                'index': index,
                'file_path': input_file,
                'pool_size': kwargs.get('pool_size', self.config['processing']['batch_size']),
                'output_file': output_path,
                'ark_url': self.config['api']['ark_url'],
                'api_key': self.config['api']['api_key'],
                'model': self.config['models']['default_model'],
                'check_task': False,
                'enhanced_quality': kwargs.get('enhanced_quality', True),
                'quality_threshold': kwargs.get('quality_threshold', 
                                              self.config['processing']['quality_threshold']),
                'user_stream': False
            }
            
            # åˆ›å»ºå‚æ•°å‘½åç©ºé—´
            class Args:
                def __init__(self, **kwargs):
                    for key, value in kwargs.items():
                        setattr(self, key, value)
            
            args = Args(**args_dict)
            
            # æ‰§è¡ŒQAç”Ÿæˆï¼ˆéœ€è¦ä¿®æ”¹åŸå‡½æ•°ä»¥æ”¯æŒç›´æ¥è°ƒç”¨ï¼‰
            results = await self.generate_qa_with_args(args)
            
            output_file = os.path.join(output_path, f"results_{index}.json")
            logger.info(f"QAç”Ÿæˆå®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} ä¸ªQAå¯¹")
            logger.info(f"ç»“æœä¿å­˜è‡³: {output_file}")
            
            return output_file
            
        except Exception as e:
            logger.error(f"QAç”Ÿæˆé˜¶æ®µå‡ºé”™: {str(e)}")
            raise
    
    async def generate_qa_with_args(self, args) -> List[Dict]:
        """ä½¿ç”¨å‚æ•°ç”ŸæˆQAï¼ˆé€‚é…åŸæœ‰å‡½æ•°ï¼‰"""
        # è¿™é‡Œéœ€è¦å¯¼å…¥å¹¶è°ƒç”¨text_qa_generationä¸­çš„æ ¸å¿ƒå‡½æ•°
        from text_qa_generation.TextQA.dataargument import get_total_responses
        
        results = await get_total_responses(
            index=args.index,
            file_path=args.file_path,
            pool_size=args.pool_size,
            stream=args.user_stream
        )
        
        # ä¿å­˜ç»“æœ
        output_file = os.path.join(args.output_file, f"results_{args.index}.json")
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=4)
            
        return results
    
    async def run_quality_control(self, input_file: str, output_path: str, **kwargs) -> str:
        """è¿è¡Œè´¨é‡æ§åˆ¶é˜¶æ®µ"""
        logger.info("=== å¼€å§‹è´¨é‡æ§åˆ¶é˜¶æ®µ ===")
        
        try:
            from text_qa_generation.TextQA.dataargument import check_data_quality
            
            # æ‰§è¡Œè´¨é‡æ£€æŸ¥
            await check_data_quality(
                ark_url=self.config['api']['ark_url'],
                api_key=self.config['api']['api_key'],
                model=self.config['models']['default_model'],
                output_file=input_file,
                check_indexes=kwargs.get('check_indexes', (40, 37, 38)),
                pool_size=kwargs.get('pool_size', self.config['processing']['batch_size']),
                check_times=kwargs.get('check_times', 9),
                stream=False
            )
            
            quality_file = input_file.replace('.json', '_quality_checked.json')
            logger.info(f"è´¨é‡æ§åˆ¶å®Œæˆï¼Œç»“æœä¿å­˜è‡³: {quality_file}")
            
            return quality_file
            
        except Exception as e:
            logger.error(f"è´¨é‡æ§åˆ¶é˜¶æ®µå‡ºé”™: {str(e)}")
            raise
    
    async def run_full_pipeline(self, input_path: str, output_path: str, 
                               domain: str = "semiconductor", **kwargs) -> Dict[str, str]:
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        logger.info("=== å¼€å§‹å®Œæ•´QAç”Ÿæˆæµæ°´çº¿ ===")
        logger.info(f"è¾“å…¥è·¯å¾„: {input_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
        logger.info(f"ä¸“ä¸šé¢†åŸŸ: {domain}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pipeline_output = os.path.join(output_path, f"pipeline_{timestamp}")
        
        directories = {
            'retrieved': os.path.join(pipeline_output, "01_retrieved"),
            'cleaned': os.path.join(pipeline_output, "02_cleaned"), 
            'qa_generated': os.path.join(pipeline_output, "03_qa_generated"),
            'quality_checked': os.path.join(pipeline_output, "04_quality_checked"),
            'final': os.path.join(pipeline_output, "05_final")
        }
        
        for directory in directories.values():
            Path(directory).mkdir(parents=True, exist_ok=True)
        
        results = {}
        
        try:
            # é˜¶æ®µ1: æ•°æ®å¬å›
            if kwargs.get('skip_retrieval', False):
                logger.info("è·³è¿‡æ•°æ®å¬å›é˜¶æ®µ")
                retrieved_file = kwargs.get('retrieved_file')
            else:
                retrieved_file = await self.run_data_retrieval(
                    input_path=input_path,
                    output_path=directories['retrieved'],
                    index=kwargs.get('retrieval_index', 43),
                    **kwargs
                )
            results['retrieved'] = retrieved_file
            
            # é˜¶æ®µ2: æ•°æ®æ¸…ç†
            cleaned_file = self.run_data_cleaning(
                input_file=retrieved_file,
                output_path=directories['cleaned'],
                **kwargs
            )
            results['cleaned'] = cleaned_file
            
            # é˜¶æ®µ3: QAç”Ÿæˆ
            domain_config = self.config['domains'].get(domain, {})
            qa_index = domain_config.get('prompts', [343])[0]
            
            qa_file = await self.run_qa_generation(
                input_file=cleaned_file,
                output_path=directories['qa_generated'],
                index=qa_index,
                **kwargs
            )
            results['qa_generated'] = qa_file
            
            # é˜¶æ®µ4: è´¨é‡æ§åˆ¶
            if kwargs.get('enable_quality_control', True):
                quality_file = await self.run_quality_control(
                    input_file=qa_file,
                    output_path=directories['quality_checked'],
                    **kwargs
                )
                results['quality_checked'] = quality_file
            else:
                results['quality_checked'] = qa_file
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = self.generate_pipeline_report(results, directories['final'])
            results['final_report'] = final_report
            
            logger.info("=== å®Œæ•´æµæ°´çº¿æ‰§è¡ŒæˆåŠŸ ===")
            logger.info(f"æœ€ç»ˆç»“æœä¿å­˜åœ¨: {pipeline_output}")
            
            return results
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise
    
    def generate_pipeline_report(self, results: Dict[str, str], output_dir: str) -> str:
        """ç”Ÿæˆæµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š"""
        report = {
            "pipeline_execution": {
                "timestamp": datetime.now().isoformat(),
                "status": "completed",
                "stages": {}
            },
            "file_locations": results,
            "statistics": {}
        }
        
        # ç»Ÿè®¡å„é˜¶æ®µæ•°æ®é‡
        for stage, file_path in results.items():
            if stage == 'final_report':
                continue
                
            try:
                if file_path.endswith('.json'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            count = len(data)
                        else:
                            count = 1
                elif file_path.endswith('.pkl'):
                    import pickle
                    with open(file_path, 'rb') as f:
                        data = pickle.load(f)
                        count = len(data) if isinstance(data, list) else 1
                else:
                    count = "unknown"
                    
                report["statistics"][stage] = {
                    "file_path": file_path,
                    "data_count": count,
                    "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0
                }
            except Exception as e:
                logger.warning(f"æ— æ³•ç»Ÿè®¡ {stage} é˜¶æ®µæ•°æ®: {str(e)}")
                report["statistics"][stage] = {
                    "file_path": file_path,
                    "data_count": "error",
                    "error": str(e)
                }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(output_dir, "pipeline_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        
        # ç”Ÿæˆç®€åŒ–ç‰ˆæŠ¥å‘Š
        summary_file = os.path.join(output_dir, "pipeline_summary.txt")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("æ™ºèƒ½QAç”Ÿæˆæµæ°´çº¿æ‰§è¡ŒæŠ¥å‘Š\n")
            f.write("=" * 40 + "\n\n")
            f.write(f"æ‰§è¡Œæ—¶é—´: {report['pipeline_execution']['timestamp']}\n")
            f.write(f"æ‰§è¡ŒçŠ¶æ€: {report['pipeline_execution']['status']}\n\n")
            
            f.write("å„é˜¶æ®µç»Ÿè®¡:\n")
            f.write("-" * 20 + "\n")
            for stage, stats in report["statistics"].items():
                f.write(f"{stage}: {stats.get('data_count', 'unknown')} æ¡æ•°æ®\n")
            
            f.write(f"\nè¯¦ç»†æŠ¥å‘Š: {report_file}\n")
        
        logger.info(f"æµæ°´çº¿æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ™ºèƒ½æ–‡æœ¬QAç”Ÿæˆç³»ç»Ÿ - ç»Ÿä¸€æµæ°´çº¿")
    
    # åŸºç¡€å‚æ•°
    parser.add_argument("--mode", type=str, default="full_pipeline",
                      choices=["full_pipeline", "retrieval", "cleaning", "qa_generation", "quality_control"],
                      help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--input_path", type=str, required=True, help="è¾“å…¥è·¯å¾„")
    parser.add_argument("--output_path", type=str, required=True, help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # é¢†åŸŸå’Œç´¢å¼•å‚æ•°
    parser.add_argument("--domain", type=str, default="semiconductor",
                      choices=["semiconductor", "optics"], help="ä¸“ä¸šé¢†åŸŸ")
    parser.add_argument("--index", type=int, default=None, help="Promptç´¢å¼•")
    parser.add_argument("--retrieval_index", type=int, default=43, help="å¬å›é˜¶æ®µç´¢å¼•")
    
    # å¤„ç†å‚æ•°
    parser.add_argument("--batch_size", type=int, default=100, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--pool_size", type=int, default=100, help="å¹¶å‘æ± å¤§å°")
    parser.add_argument("--selected_task_number", type=int, default=1000, help="é€‰æ‹©å¤„ç†çš„ä»»åŠ¡æ•°")
    parser.add_argument("--quality_threshold", type=float, default=0.7, help="è´¨é‡é˜ˆå€¼")
    
    # æ§åˆ¶å‚æ•°
    parser.add_argument("--skip_retrieval", action="store_true", help="è·³è¿‡æ•°æ®å¬å›")
    parser.add_argument("--retrieved_file", type=str, help="å·²å¬å›çš„æ•°æ®æ–‡ä»¶")
    parser.add_argument("--enable_quality_control", action="store_true", default=True, 
                      help="å¯ç”¨è´¨é‡æ§åˆ¶")
    parser.add_argument("--enhanced_quality", action="store_true", default=True,
                      help="ä½¿ç”¨å¢å¼ºè´¨é‡æ£€æŸ¥")
    parser.add_argument("--copy_parsed_pdf", action="store_true", help="å¤åˆ¶è§£æçš„PDF")
    
    # è°ƒè¯•å‚æ•°
    parser.add_argument("--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("--dry_run", action="store_true", help="è¯•è¿è¡Œï¼ˆä¸æ‰§è¡Œå®é™…å¤„ç†ï¼‰")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—çº§åˆ«
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = IntelligentQAPipeline(args.config)
    
    if args.dry_run:
        logger.info("è¯•è¿è¡Œæ¨¡å¼ - ä¸æ‰§è¡Œå®é™…å¤„ç†")
        logger.info(f"é…ç½®: {json.dumps(pipeline.config, indent=2, ensure_ascii=False)}")
        return
    
    # æ‰§è¡Œç›¸åº”æ¨¡å¼
    try:
        if args.mode == "full_pipeline":
            results = await pipeline.run_full_pipeline(
                input_path=args.input_path,
                output_path=args.output_path,
                domain=args.domain,
                batch_size=args.batch_size,
                pool_size=args.pool_size,
                selected_task_number=args.selected_task_number,
                quality_threshold=args.quality_threshold,
                skip_retrieval=args.skip_retrieval,
                retrieved_file=args.retrieved_file,
                enable_quality_control=args.enable_quality_control,
                enhanced_quality=args.enhanced_quality,
                copy_parsed_pdf=args.copy_parsed_pdf,
                retrieval_index=args.retrieval_index
            )
            print(f"\nâœ… æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
            print(f"ğŸ“Š æœ€ç»ˆæŠ¥å‘Š: {results.get('final_report')}")
            
        elif args.mode == "retrieval":
            result_file = await pipeline.run_data_retrieval(
                input_path=args.input_path,
                output_path=args.output_path,
                index=args.retrieval_index,
                batch_size=args.batch_size,
                selected_task_number=args.selected_task_number
            )
            print(f"\nâœ… æ•°æ®å¬å›å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
            
        elif args.mode == "cleaning":
            result_file = pipeline.run_data_cleaning(
                input_file=args.input_path,
                output_path=args.output_path,
                copy_parsed_pdf=args.copy_parsed_pdf
            )
            print(f"\nâœ… æ•°æ®æ¸…ç†å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
            
        elif args.mode == "qa_generation":
            qa_index = args.index or pipeline.config['domains'][args.domain]['prompts'][0]
            result_file = await pipeline.run_qa_generation(
                input_file=args.input_path,
                output_path=args.output_path,
                index=qa_index,
                pool_size=args.pool_size,
                enhanced_quality=args.enhanced_quality,
                quality_threshold=args.quality_threshold
            )
            print(f"\nâœ… QAç”Ÿæˆå®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
            
        elif args.mode == "quality_control":
            result_file = await pipeline.run_quality_control(
                input_file=args.input_path,
                output_path=args.output_path,
                pool_size=args.pool_size
            )
            print(f"\nâœ… è´¨é‡æ§åˆ¶å®Œæˆï¼ç»“æœæ–‡ä»¶: {result_file}")
            
    except Exception as e:
        logger.error(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())