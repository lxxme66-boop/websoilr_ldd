# 使用本地 Skywork-R1V3-38B 模型配置说明

## 需要修改的地方

1. 在 config.json 中修改 API 配置：
   - ark_url: 改为 http://localhost:8080/v1
   - use_local_models: 确保为 true
   - local_model_priority: 改为 ["vllm", "transformers", "ollama"]

2. 在 config.json 中修改 vLLM 配置：
   - enabled: 改为 true
   - base_url: 改为 http://localhost:8080
   - model_name: 改为 /mnt/storage/models/Skywork/Skywork-R1V3-38B

3. 使用 vLLM 启动本地模型服务：
   python -m vllm.entrypoints.openai.api_server --model /mnt/storage/models/Skywork/Skywork-R1V3-38B --host 0.0.0.0 --port 8080

4. 运行程序：
   python run_pipeline.py --use_local_models

为什么需要 setup_ollama.sh：
- 提供备用方案，当主模型不可用时使用
- 多层次容错设计，确保系统高可用性
