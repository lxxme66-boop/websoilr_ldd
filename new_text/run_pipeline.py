#!/usr/bin/env python3
"""
æ™ºèƒ½æ–‡æœ¬QAç”Ÿæˆç³»ç»Ÿ - æ•´åˆç‰ˆç»Ÿä¸€æµæ°´çº¿
æ•´åˆäº†æ•°æ®å¬å›ã€æ¸…ç†ã€QAç”Ÿæˆã€è´¨é‡æ§åˆ¶ã€å¤šæ¨¡æ€å¤„ç†å’Œæœ¬åœ°æ¨¡å‹æ”¯æŒçš„å®Œæ•´æµæ°´çº¿

åŠŸèƒ½æ¨¡å—ï¼š
1. æ–‡æœ¬å¬å›ä¸æ£€ç´¢ (Text Retrieval)
2. æ•°æ®æ¸…ç†ä¸é¢„å¤„ç† (Data Cleaning)  
3. æ™ºèƒ½QAç”Ÿæˆ (QA Generation)
4. å¢å¼ºè´¨é‡æ§åˆ¶ (Quality Control)
5. å¤šæ¨¡æ€å¤„ç† (Multimodal Processing)
6. æœ¬åœ°æ¨¡å‹æ”¯æŒ (Local Models)
"""

import asyncio
import argparse
import json
import os
import sys
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union
import time
from datetime import datetime

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from text_main_batch_inference_enhanced import main as retrieval_main
from clean_data import main as cleaning_main
from text_qa_generation import main as qa_generation_main
from TextQA.enhanced_quality_checker import TextQAQualityIntegrator

# å¯¼å…¥å¤šæ¨¡æ€å’Œæœ¬åœ°æ¨¡å‹æ”¯æŒ
try:
    from MultiModal.pdf_processor import PDFProcessor
    MULTIMODAL_AVAILABLE = True
except ImportError:
    MULTIMODAL_AVAILABLE = False
    print("Warning: MultiModal processing not available")

try:
    from LocalModels.ollama_client import OllamaClient
    LOCAL_MODELS_AVAILABLE = True
except ImportError:
    LOCAL_MODELS_AVAILABLE = False
    print("Warning: Local models not available")

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/pipeline.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class IntegratedQAPipeline:
    """æ•´åˆç‰ˆQAç”Ÿæˆæµæ°´çº¿"""
    
    def __init__(self, config_path: str = "config.json"):
        """åˆå§‹åŒ–æµæ°´çº¿"""
        self.config_path = config_path
        self.config = self.load_config()
        self.setup_directories()
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'start_time': None,
            'end_time': None,
            'total_duration': 0,
            'stages_completed': [],
            'total_files_processed': 0,
            'total_qa_pairs_generated': 0,
            'quality_pass_rate': 0.0,
            'errors': []
        }
    
    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {self.config_path}")
            return config
        except Exception as e:
            logger.error(f"é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
            raise
    
    def setup_directories(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
        directories = [
            self.config['file_paths']['input']['base_dir'],
            self.config['file_paths']['output']['base_dir'],
            self.config['file_paths']['output']['retrieved_dir'],
            self.config['file_paths']['output']['cleaned_dir'],
            self.config['file_paths']['output']['qa_dir'],
            self.config['file_paths']['output']['quality_dir'],
            self.config['file_paths']['output']['final_dir'],
            self.config['file_paths']['temp']['base_dir'],
            self.config['file_paths']['temp']['cache_dir'],
            self.config['file_paths']['temp']['logs_dir']
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
        
        logger.info("ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ")
    
    async def run_full_pipeline(self, input_path: str, domain: str = "semiconductor") -> Dict:
        """è¿è¡Œå®Œæ•´æµæ°´çº¿"""
        logger.info("=" * 60)
        logger.info("å¼€å§‹è¿è¡Œæ™ºèƒ½æ–‡æœ¬QAç”Ÿæˆç³»ç»Ÿ - æ•´åˆç‰ˆ")
        logger.info("=" * 60)
        
        self.stats['start_time'] = datetime.now()
        
        try:
            # é˜¶æ®µ1: æ–‡æœ¬å¬å›
            if os.path.isdir(input_path):
                retrieval_results = await self.stage_text_retrieval(input_path, domain)
                self.stats['stages_completed'].append('text_retrieval')
            else:
                logger.info("è·³è¿‡æ–‡æœ¬å¬å›é˜¶æ®µ - è¾“å…¥ä¸ºå•ä¸ªæ–‡ä»¶")
                retrieval_results = {'output_file': input_path}
            
            # é˜¶æ®µ2: æ•°æ®æ¸…ç†
            cleaning_results = await self.stage_data_cleaning(
                retrieval_results.get('output_file', input_path)
            )
            self.stats['stages_completed'].append('data_cleaning')
            
            # é˜¶æ®µ3: QAç”Ÿæˆ
            qa_results = await self.stage_qa_generation(
                cleaning_results['output_file'], domain
            )
            self.stats['stages_completed'].append('qa_generation')
            
            # é˜¶æ®µ4: è´¨é‡æ§åˆ¶
            quality_results = await self.stage_quality_control(
                qa_results['output_file']
            )
            self.stats['stages_completed'].append('quality_control')
            
            # é˜¶æ®µ5: æœ€ç»ˆæ•´ç†
            final_results = await self.stage_final_processing(
                quality_results['output_file'], domain
            )
            self.stats['stages_completed'].append('final_processing')
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            report = self.generate_final_report(final_results)
            
            self.stats['end_time'] = datetime.now()
            self.stats['total_duration'] = (
                self.stats['end_time'] - self.stats['start_time']
            ).total_seconds()
            
            logger.info("=" * 60)
            logger.info("æµæ°´çº¿æ‰§è¡Œå®Œæˆ")
            logger.info(f"æ€»è€—æ—¶: {self.stats['total_duration']:.2f} ç§’")
            logger.info("=" * 60)
            
            return {
                'success': True,
                'final_output': final_results,
                'report': report,
                'stats': self.stats
            }
            
        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            self.stats['errors'].append(str(e))
            return {
                'success': False,
                'error': str(e),
                'stats': self.stats
            }
    
    async def stage_text_retrieval(self, input_path: str, domain: str) -> Dict:
        """é˜¶æ®µ1: æ–‡æœ¬å¬å›"""
        logger.info("é˜¶æ®µ1: å¼€å§‹æ–‡æœ¬å¬å›...")
        
        output_dir = self.config['file_paths']['output']['retrieved_dir']
        
        # æ™ºèƒ½åˆ¤æ–­è¾“å…¥è·¯å¾„ç±»å‹
        actual_input_path = input_path
        
        # å¦‚æœè¾“å…¥è·¯å¾„æ˜¯data/pdfsä½†ä¸å­˜åœ¨ï¼Œæ£€æŸ¥data/texts
        if input_path == "data/pdfs" and not os.path.exists(input_path):
            if os.path.exists("data/texts"):
                logger.info("PDFç›®å½•ä¸å­˜åœ¨ï¼Œåˆ‡æ¢åˆ°æ–‡æœ¬ç›®å½•: data/texts")
                actual_input_path = "data/texts"
            else:
                # åˆ›å»ºpdfsç›®å½•
                os.makedirs("data/pdfs", exist_ok=True)
                logger.warning(f"åˆ›å»ºPDFç›®å½•: {input_path}")
        
        # å¦‚æœè·¯å¾„åŒ…å«pdfä½†ç›®å½•ä¸ºç©ºï¼Œå°è¯•textsç›®å½•
        if "pdf" in input_path.lower():
            pdf_files = []
            if os.path.exists(input_path):
                pdf_files = [f for f in os.listdir(input_path) if f.endswith('.pdf')]
            
            if not pdf_files and os.path.exists("data/texts"):
                txt_files = [f for f in os.listdir("data/texts") if f.endswith('.txt')]
                if txt_files:
                    logger.info(f"PDFç›®å½•ä¸ºç©ºï¼Œå‘ç°æ–‡æœ¬æ–‡ä»¶åœ¨data/textsï¼Œåˆ‡æ¢å¤„ç†")
                    actual_input_path = "data/texts"
        
        # æ„å»ºå¬å›å‚æ•°
        retrieval_args = {
            'index': self.config['data_retrieval']['retrieval_indices'].get(domain, 43),
            'parallel_batch_size': self.config['processing']['parallel_batch_size'],
            'pdf_path': actual_input_path,
            'storage_folder': output_dir,
            'selected_task_number': self.config['processing']['selected_task_number'],
            'read_hist': False
        }
        
        logger.info(f"å®é™…å¤„ç†è·¯å¾„: {actual_input_path}")
        
        try:
            # è°ƒç”¨å¬å›æ¨¡å—
            await retrieval_main(
                index=retrieval_args['index'],
                parallel_batch_size=retrieval_args['parallel_batch_size'],
                pdf_path=retrieval_args['pdf_path'],
                storage_folder=retrieval_args['storage_folder'],
                selected_task_number=retrieval_args['selected_task_number'],
                read_hist=retrieval_args['read_hist']
            )
            
            output_file = os.path.join(output_dir, 'total_response.pkl')
            logger.info(f"æ–‡æœ¬å¬å›å®Œæˆï¼Œè¾“å‡º: {output_file}")
            
            return {
                'success': True,
                'output_file': output_file,
                'stage': 'text_retrieval'
            }
        except Exception as e:
            logger.error(f"æ–‡æœ¬å¬å›å¤±è´¥: {e}")
            raise
    
    async def stage_data_cleaning(self, input_file: str) -> Dict:
        """é˜¶æ®µ2: æ•°æ®æ¸…ç†"""
        logger.info("é˜¶æ®µ2: å¼€å§‹æ•°æ®æ¸…ç†...")
        
        output_dir = self.config['file_paths']['output']['cleaned_dir']
        
        try:
            # è°ƒç”¨æ¸…ç†æ¨¡å—
            cleaned_file = await cleaning_main(
                input_file=input_file,
                output_dir=output_dir,
                copy_parsed_pdf=False
            )
            
            logger.info(f"æ•°æ®æ¸…ç†å®Œæˆï¼Œè¾“å‡º: {cleaned_file}")
            
            return {
                'success': True,
                'output_file': cleaned_file,
                'stage': 'data_cleaning'
            }
        except Exception as e:
            logger.error(f"æ•°æ®æ¸…ç†å¤±è´¥: {e}")
            raise
    
    async def stage_qa_generation(self, input_file: str, domain: str) -> Dict:
        """é˜¶æ®µ3: QAç”Ÿæˆ"""
        logger.info("é˜¶æ®µ3: å¼€å§‹QAç”Ÿæˆ...")
        
        output_dir = self.config['file_paths']['output']['qa_dir']
        
        # è·å–é¢†åŸŸç‰¹å®šçš„prompt
        prompt_index = self.config['professional_domains']['domain_specific_prompts'].get(
            domain, [343]
        )[0]
        
        try:
            # æ„å»ºQAç”Ÿæˆå‚æ•°
            qa_args = {
                'index': prompt_index,
                'file_path': input_file,
                'pool_size': self.config['processing']['pool_size'],
                'output_file': output_dir,
                'ark_url': self.config['api']['ark_url'],
                'api_key': self.config['api']['api_key'],
                'model': self.config['models']['qa_generator_model']['path'],
                'check_task': False,
                'user_stream': False,
                'enhanced_quality': False
            }
            
            # è°ƒç”¨QAç”Ÿæˆæ¨¡å—
            results = await qa_generation_main(**qa_args)
            
            output_file = os.path.join(output_dir, f"results_{prompt_index}.json")
            self.stats['total_qa_pairs_generated'] = len(results) if results else 0
            
            logger.info(f"QAç”Ÿæˆå®Œæˆï¼Œç”Ÿæˆ {self.stats['total_qa_pairs_generated']} ä¸ªQAå¯¹")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {output_file}")
            
            return {
                'success': True,
                'output_file': output_file,
                'qa_count': self.stats['total_qa_pairs_generated'],
                'stage': 'qa_generation'
            }
        except Exception as e:
            logger.error(f"QAç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def stage_quality_control(self, input_file: str) -> Dict:
        """é˜¶æ®µ4: è´¨é‡æ§åˆ¶"""
        logger.info("é˜¶æ®µ4: å¼€å§‹è´¨é‡æ§åˆ¶...")
        
        output_dir = self.config['file_paths']['output']['quality_dir']
        
        try:
            # ä½¿ç”¨å¢å¼ºè´¨é‡æ£€æŸ¥
            integrator = TextQAQualityIntegrator(self.config)
            
            quality_report = await integrator.enhanced_quality_check(
                qa_file_path=input_file,
                output_dir=output_dir,
                quality_threshold=self.config['quality_control']['enhanced_quality_check']['quality_threshold']
            )
            
            self.stats['quality_pass_rate'] = quality_report.get('pass_rate', 0.0)
            
            logger.info("è´¨é‡æ§åˆ¶å®Œæˆ")
            logger.info(f"è´¨é‡é€šè¿‡ç‡: {self.stats['quality_pass_rate']:.2%}")
            
            output_file = os.path.join(output_dir, "quality_checked_qa.json")
            
            return {
                'success': True,
                'output_file': output_file,
                'quality_report': quality_report,
                'stage': 'quality_control'
            }
        except Exception as e:
            logger.error(f"è´¨é‡æ§åˆ¶å¤±è´¥: {e}")
            raise
    
    async def stage_final_processing(self, input_file: str, domain: str) -> Dict:
        """é˜¶æ®µ5: æœ€ç»ˆå¤„ç†"""
        logger.info("é˜¶æ®µ5: å¼€å§‹æœ€ç»ˆå¤„ç†...")
        
        final_dir = self.config['file_paths']['output']['final_dir']
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # è¯»å–è´¨é‡æ£€æŸ¥åçš„æ•°æ®
            with open(input_file, 'r', encoding='utf-8') as f:
                qa_data = json.load(f)
            
            # æ·»åŠ å…ƒæ•°æ®
            final_data = {
                'metadata': {
                    'system_name': self.config['system_info']['name'],
                    'version': self.config['system_info']['version'],
                    'domain': domain,
                    'generation_time': timestamp,
                    'total_qa_pairs': len(qa_data) if isinstance(qa_data, list) else qa_data.get('total_qa_pairs', 0),
                    'quality_threshold': self.config['quality_control']['enhanced_quality_check']['quality_threshold'],
                    'quality_pass_rate': self.stats['quality_pass_rate']
                },
                'qa_pairs': qa_data,
                'pipeline_stats': self.stats
            }
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            final_output_file = os.path.join(
                final_dir, 
                f"final_qa_results_{domain}_{timestamp}.json"
            )
            
            with open(final_output_file, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=4)
            
            logger.info(f"æœ€ç»ˆå¤„ç†å®Œæˆï¼Œè¾“å‡º: {final_output_file}")
            
            return {
                'success': True,
                'output_file': final_output_file,
                'final_data': final_data,
                'stage': 'final_processing'
            }
        except Exception as e:
            logger.error(f"æœ€ç»ˆå¤„ç†å¤±è´¥: {e}")
            raise
    
    def generate_final_report(self, final_results: Dict) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        report = {
            'pipeline_summary': {
                'system_name': self.config['system_info']['name'],
                'version': self.config['system_info']['version'],
                'execution_time': self.stats['total_duration'],
                'stages_completed': self.stats['stages_completed'],
                'success': len(self.stats['errors']) == 0
            },
            'processing_stats': {
                'total_files_processed': self.stats['total_files_processed'],
                'total_qa_pairs_generated': self.stats['total_qa_pairs_generated'],
                'quality_pass_rate': self.stats['quality_pass_rate']
            },
            'output_files': {
                'final_output': final_results.get('output_file'),
                'logs': 'logs/pipeline.log'
            },
            'errors': self.stats['errors']
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = os.path.join(
            self.config['file_paths']['output']['final_dir'],
            f"pipeline_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=4)
        
        logger.info(f"æµæ°´çº¿æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report

    async def run_single_stage(self, stage: str, input_path: str, **kwargs) -> Dict:
        """è¿è¡Œå•ä¸ªé˜¶æ®µ"""
        logger.info(f"è¿è¡Œå•ä¸ªé˜¶æ®µ: {stage}")
        
        if stage == "text_retrieval":
            return await self.stage_text_retrieval(input_path, kwargs.get('domain', 'semiconductor'))
        elif stage == "data_cleaning":
            return await self.stage_data_cleaning(input_path)
        elif stage == "qa_generation":
            return await self.stage_qa_generation(input_path, kwargs.get('domain', 'semiconductor'))
        elif stage == "quality_control":
            return await self.stage_quality_control(input_path)
        elif stage == "final_processing":
            return await self.stage_final_processing(input_path, kwargs.get('domain', 'semiconductor'))
        else:
            raise ValueError(f"æœªçŸ¥çš„é˜¶æ®µ: {stage}")

async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="æ™ºèƒ½æ–‡æœ¬QAç”Ÿæˆç³»ç»Ÿ - æ•´åˆç‰ˆ")
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument("--mode", type=str, default="full_pipeline", 
                       choices=["full_pipeline", "text_retrieval", "data_cleaning", 
                               "qa_generation", "quality_control", "final_processing"],
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--input_path", type=str, required=True, help="è¾“å…¥è·¯å¾„")
    parser.add_argument("--output_path", type=str, help="è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--config", type=str, default="config.json", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--domain", type=str, default="semiconductor", 
                       choices=["semiconductor", "optics", "materials", "physics", "chemistry"],
                       help="ä¸“ä¸šé¢†åŸŸ")
    
    # é«˜çº§å‚æ•°
    parser.add_argument("--batch_size", type=int, help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--pool_size", type=int, help="å¹¶å‘æ± å¤§å°")
    parser.add_argument("--quality_threshold", type=float, help="è´¨é‡é˜ˆå€¼")
    parser.add_argument("--use_local_models", action="store_true", help="ä½¿ç”¨æœ¬åœ°æ¨¡å‹")
    parser.add_argument("--enable_multimodal", action="store_true", help="å¯ç”¨å¤šæ¨¡æ€å¤„ç†")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æµæ°´çº¿
    pipeline = IntegratedQAPipeline(args.config)
    
    # æ›´æ–°é…ç½®ï¼ˆå¦‚æœæä¾›äº†å‚æ•°ï¼‰
    if args.batch_size:
        pipeline.config['processing']['batch_size'] = args.batch_size
    if args.pool_size:
        pipeline.config['processing']['pool_size'] = args.pool_size
    if args.quality_threshold:
        pipeline.config['quality_control']['enhanced_quality_check']['quality_threshold'] = args.quality_threshold
    if args.use_local_models:
        pipeline.config['api']['use_local_models'] = True
    if args.enable_multimodal:
        pipeline.config['multimodal']['enabled'] = True
    
    try:
        if args.mode == "full_pipeline":
            # è¿è¡Œå®Œæ•´æµæ°´çº¿
            results = await pipeline.run_full_pipeline(args.input_path, args.domain)
        else:
            # è¿è¡Œå•ä¸ªé˜¶æ®µ
            results = await pipeline.run_single_stage(
                args.mode, args.input_path, domain=args.domain
            )
        
        if results['success']:
            print("\n" + "=" * 60)
            print("âœ… æµæ°´çº¿æ‰§è¡ŒæˆåŠŸï¼")
            if 'final_output' in results:
                print(f"ğŸ“ æœ€ç»ˆè¾“å‡º: {results['final_output']['output_file']}")
            if 'report' in results:
                print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: ç”Ÿæˆ {results['stats']['total_qa_pairs_generated']} ä¸ªQAå¯¹")
                print(f"â­ è´¨é‡é€šè¿‡ç‡: {results['stats']['quality_pass_rate']:.2%}")
            print("=" * 60)
        else:
            print(f"\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {results['error']}")
            return 1
            
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        print(f"\nğŸ’¥ ç¨‹åºæ‰§è¡Œå¼‚å¸¸: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    import sys
    sys.exit(asyncio.run(main()))